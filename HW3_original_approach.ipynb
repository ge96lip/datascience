{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will preprocess the dataset and perform some basic regression and classification tasks. The learning outcome of this part is to know how one can pre-process a real-world dataset and perform a supervised learning task, and to understand some of the fundamental mechanisms behind these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student information\n",
    "Please provide your information for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUD_SUID = 'lika3203'\n",
    "STUD_NAME = 'Linn Karlsson'\n",
    "STUD_EMAIL = 'lika3203@student.su.se'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grading: \n",
    "\n",
    "Pass/Fail.\n",
    "\n",
    "To Pass this HW you need to provide a complete and correct solution, where one minor mistake is allowed. However, if your solution has more minor mistakes or lacks parts entirely or has one or more major mistakes, then you receive a Fail grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE: \n",
    "\n",
    "Data pre-processing, regression task and classification task\n",
    "\n",
    "1. Reading the files\n",
    "2. Missing Values\n",
    "3. Imputing categorical variables\n",
    "4. Imputing numerical variables\n",
    "5. Classification with Decision Tree, single split\n",
    "6. Classification with Decision Tree, Cross validation\n",
    "7. Interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important instructions:\n",
    "\n",
    "Each function you make will be considered during the grading, so it is important to strictly follow input and output instructions stated in the skeleton code.\n",
    "\n",
    "You must not delete any of the given cells or change the structure of the cells or change the instructions in the cells or add cells (unless completely necessary, add a comment on why you added a cell) as they will help in grading the assignment. Should you contravene this provision, you will fail the assignment, and no feedback will be given on the part after the contravention.\n",
    "\n",
    "Some variable names are already given and have random values or empty arrays assigned on them. In this case you should only change the assignments on the variables but keep the names as given.\n",
    "\n",
    "When you are finished with implementing all the tasks, **clear all outputs, run all cells again** (make sure there is no error) and submit!\n",
    "\n",
    "Make sure that the results and figures asked are visible for us to grade.\n",
    "\n",
    "Make sure not to modify the files in the \"data\" folder in your submission, and not to change the folder structure or the files location, or your submission will not obtain a passing grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure consistent results, make sure that every operation in which you can use a random seed has it set to 8. If your process is correct, but the results are wrong due to the seed being wrong, it will be considered a major mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.venv/lib/python3.11/site-packages (from seaborn) (2.1.2)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.venv/lib/python3.11/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.venv/lib/python3.11/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# added this cell because I am running with a clean .venv and don't have any libraries installed yet\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the libraries that you will need throughout the assignment\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "RSEED = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1.* Reading the files\n",
    "\n",
    "### `Task: Read the datasets using pandas. Use the files called cleveland.data and switzerland.data that you have downloaded in this archive.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets contain information about adult patients from the US and from Switzerland. You can find more information in the heart-disease.names file in the 'data' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the folder 'data', read the files cleveland.data and switzerland.data into the dataframes cleveland and test, respectively.\n",
    "# Make sure to add the names of the variables to both dataframes.\n",
    "\n",
    "columns = ['age', 'sex', 'cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num'] # you can find the column names in the file 'data/heart-disease.names'.\n",
    "# Select the correct column names for the dataset, as described in the file.\n",
    "\n",
    "\n",
    "# cleveland = pd.read_csv(\"../data/cleveland.data\", names=columns)\n",
    "# test = pd.read_csv(\"../data/switzerland.data\", names=columns)\n",
    "\n",
    "cleveland = pd.read_csv(\"./cleveland.data\", names=columns)\n",
    "test = pd.read_csv(\"./switzerland.data\", names=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "\n",
       "   slope   ca thal  num  \n",
       "0    3.0  0.0  6.0    0  \n",
       "1    2.0  3.0  3.0    2  \n",
       "2    2.0  2.0  7.0    1  \n",
       "3    3.0  0.0  3.0    0  \n",
       "4    1.0  0.0  3.0    0  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not delete this!\n",
    "cleveland.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps  chol fbs restecg  thalach  exang  oldpeak slope  \\\n",
       "0  32.0  1.0  1.0      95.0   0.0   ?       0    127.0    0.0      0.7     1   \n",
       "1  34.0  1.0  4.0     115.0   0.0   ?       ?    154.0    0.0      0.2     1   \n",
       "2  36.0  1.0  4.0     110.0   0.0   ?       0    125.0    1.0      1.0     2   \n",
       "3  38.0  0.0  4.0     105.0   0.0   ?       0    166.0    0.0      2.8     1   \n",
       "4  38.0  0.0  4.0     110.0   0.0   0       0    156.0    0.0      0.0     2   \n",
       "\n",
       "  ca thal  num  \n",
       "0  ?    ?  1.0  \n",
       "1  ?    ?  1.0  \n",
       "2  ?    6  1.0  \n",
       "3  ?    ?  2.0  \n",
       "4  ?    3  1.0  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not delete this!\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.270627</td>\n",
       "      <td>0.679868</td>\n",
       "      <td>3.158416</td>\n",
       "      <td>131.689769</td>\n",
       "      <td>246.693069</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>149.607261</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.600660</td>\n",
       "      <td>0.937294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>77.296578</td>\n",
       "      <td>0.467299</td>\n",
       "      <td>0.960126</td>\n",
       "      <td>17.599748</td>\n",
       "      <td>51.776918</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.994971</td>\n",
       "      <td>22.875003</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>1.228536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    60.270627    0.679868    3.158416  131.689769  246.693069    0.148515   \n",
       "std     77.296578    0.467299    0.960126   17.599748   51.776918    0.356198   \n",
       "min      0.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
       "25%     47.500000    0.000000    3.000000  120.000000  211.000000    0.000000   \n",
       "50%     55.000000    1.000000    3.000000  130.000000  241.000000    0.000000   \n",
       "75%     61.000000    1.000000    4.000000  140.000000  275.000000    0.000000   \n",
       "max    999.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope         num  \n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000  \n",
       "mean     0.990099  149.607261    0.326733    1.039604    1.600660    0.937294  \n",
       "std      0.994971   22.875003    0.469794    1.161075    0.616226    1.228536  \n",
       "min      0.000000   71.000000    0.000000    0.000000    1.000000    0.000000  \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000  \n",
       "50%      1.000000  153.000000    0.000000    0.800000    2.000000    0.000000  \n",
       "75%      2.000000  166.000000    1.000000    1.600000    2.000000    2.000000  \n",
       "max      2.000000  202.000000    1.000000    6.200000    3.000000    4.000000  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to see information about the dataset, uncomment:\n",
    "cleveland.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>82.409836</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>3.683761</td>\n",
       "      <td>129.957265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.299145</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>1.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>170.211621</td>\n",
       "      <td>0.280782</td>\n",
       "      <td>0.702822</td>\n",
       "      <td>22.423200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.759921</td>\n",
       "      <td>0.498007</td>\n",
       "      <td>1.056061</td>\n",
       "      <td>1.011866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps   chol     thalach  \\\n",
       "count  122.000000  117.000000  117.000000  117.000000  117.0  117.000000   \n",
       "mean    82.409836    0.914530    3.683761  129.957265    0.0  122.299145   \n",
       "std    170.211621    0.280782    0.702822   22.423200    0.0   25.759921   \n",
       "min      0.000000    0.000000    1.000000   80.000000    0.0   60.000000   \n",
       "25%     48.500000    1.000000    4.000000  115.000000    0.0  105.000000   \n",
       "50%     56.000000    1.000000    4.000000  125.000000    0.0  121.000000   \n",
       "75%     61.000000    1.000000    4.000000  145.000000    0.0  141.000000   \n",
       "max    999.000000    1.000000    4.000000  200.000000    0.0  182.000000   \n",
       "\n",
       "            exang     oldpeak         num  \n",
       "count  117.000000  117.000000  117.000000  \n",
       "mean     0.435897    0.653846    1.769231  \n",
       "std      0.498007    1.056061    1.011866  \n",
       "min      0.000000   -2.600000    0.000000  \n",
       "25%      0.000000    0.000000    1.000000  \n",
       "50%      0.000000    0.300000    2.000000  \n",
       "75%      1.000000    1.500000    3.000000  \n",
       "max      1.000000    3.700000    4.000000  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to see information about the dataset, uncomment:\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2.* Missing values\n",
    "\n",
    "### `Task: Produce a plot with two subplots, each showing a bar plot of the 'missing' values (either encoded as NaN, or encoded with values that should not be in the dataset) for each feature for the two dataframes. The plot must have a name, and the bars must be named using the feature names.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(age         4\n",
       " sex         0\n",
       " cp          0\n",
       " trestbps    0\n",
       " chol        0\n",
       " fbs         0\n",
       " restecg     0\n",
       " thalach     0\n",
       " exang       0\n",
       " oldpeak     0\n",
       " slope       0\n",
       " ca          4\n",
       " thal        7\n",
       " num         0\n",
       " dtype: int64,\n",
       " age           7\n",
       " sex           0\n",
       " cp            0\n",
       " trestbps      0\n",
       " chol          0\n",
       " fbs          71\n",
       " restecg       1\n",
       " thalach       0\n",
       " exang         0\n",
       " oldpeak       0\n",
       " slope        11\n",
       " ca          112\n",
       " thal         51\n",
       " num           0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and display the number of missing values for each column in Cleveland and Switzerland datasets\n",
    "# Internet look up of the dataset showed that the original age range is 29-77 (https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset) \n",
    "# Therefore the question arises if the rows with ages outside this range should be dropped or imputed \n",
    "# The decision was made to drope the rows which have an invalid age + only NaN for the other variables -> infeasible to impute any values \n",
    "# For the rows with ages outside the rows but values for at least one other variable the age was set to NaN and all missing values are imputed later on\n",
    "\n",
    "cleveland.replace(['?', '!'], np.nan, inplace=True)\n",
    "cleveland['age'] = cleveland['age'].replace({0: np.nan, 999: np.nan})\n",
    "test.replace(['?', '!'], np.nan, inplace=True)\n",
    "test['age'] = test['age'].replace({0: np.nan, 999: np.nan})\n",
    "\n",
    "\n",
    "def handle_invalid_age(df):\n",
    "    df['age'] = df['age'].replace({0: np.nan, 999: np.nan})\n",
    "    return df\n",
    "\n",
    "# Function to drop rows where all columns are missing\n",
    "def drop_rows_with_all_missing_numerical(df):\n",
    "    df = df.dropna(how='all')\n",
    "    return df\n",
    "\n",
    "#cleveland = handle_invalid_age(cleveland)\n",
    "cleveland = drop_rows_with_all_missing_numerical(cleveland)\n",
    "#test = handle_invalid_age(test)\n",
    "test = drop_rows_with_all_missing_numerical(test)\n",
    "\n",
    "miss_cleveland = cleveland.isna().sum()\n",
    "miss_test = test.isna().sum()\n",
    "\n",
    "miss_cleveland, miss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Missing values of Cleveland & Switzerland data')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create 1 figure with a set of 2 subplots. Each axes should contain a figure as described below: \n",
    "# subplot 1: A barplot with the missing valies for each attribute in the dataset 'cleveland'\n",
    "# subplot 2: A barplot with the missing valies for each attribute in the dataset 'test'\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 9))\n",
    "\n",
    "miss_cleveland.plot(kind='bar', ax=ax[0], color='darkgreen')\n",
    "miss_test.plot(kind='bar', ax=ax[1], color='orange')\n",
    "\n",
    "fig.suptitle('Missing values of Cleveland & Switzerland data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3.* Imputing categorical variables\n",
    "\n",
    "In the file 'data/heart-disease.names' you can find, together with the names of the columns, a description of their contents.\n",
    "\n",
    "Determine which columns are categorical, and set their type to object.\n",
    "\n",
    "Determine which columns are numerical, and set their type accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         float64\n",
       "sex          object\n",
       "cp           object\n",
       "trestbps    float64\n",
       "chol        float64\n",
       "fbs          object\n",
       "restecg      object\n",
       "thalach     float64\n",
       "exang        object\n",
       "oldpeak     float64\n",
       "slope        object\n",
       "ca           object\n",
       "thal         object\n",
       "num           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleveland.info()\n",
    "# test.info()\n",
    "\n",
    "categorical_columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "numerical_columns_int = ['num']\n",
    "numerical_columns_float = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "cleveland[categorical_columns] = cleveland[categorical_columns].astype('object')\n",
    "test[categorical_columns] = test[categorical_columns].astype('object')\n",
    "\n",
    "# Verify the type changes: \n",
    "cleveland.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Split the cleveland dataframe in a train and a validation set. `\n",
    "\n",
    "The train set must be called train, the the validation set must be called val. The size of the validation set must be 30% of the total size of the cleveland dataframe. Use shuffle=True and stratify=True. Make sure that both train and val are dataframes, and that the columns have the correct names. Reset the indexes of all four the dataframes, using drop=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your original code: \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X_cleveland = cleveland.drop(columns=[\"num\"]) \n",
    "y_cleveland = cleveland[\"num\"]\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_cleveland, y_cleveland, test_size=0.3, shuffle=True, stratify=y_cleveland\n",
    ")\n",
    "\n",
    "# Combine features and target for train and validation sets\n",
    "train = X_train.assign(num=y_train.values)\n",
    "val = X_val.assign(num=y_val.values)\n",
    "\n",
    "# Reset the indexes of train and val dataframes\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp  trestbps   chol  fbs restecg  thalach exang  oldpeak  \\\n",
       "285  58.0  1.0  4.0     114.0  318.0  0.0     1.0    140.0   0.0      4.4   \n",
       "193  62.0  0.0  4.0     138.0  294.0  1.0     0.0    106.0   0.0      1.9   \n",
       "266  52.0  1.0  4.0     128.0  204.0  1.0     0.0    156.0   1.0      1.0   \n",
       "279  58.0  0.0  4.0     130.0  197.0  0.0     0.0    131.0   0.0      0.6   \n",
       "120  48.0  1.0  4.0     130.0  256.0  1.0     2.0    150.0   1.0      0.0   \n",
       "\n",
       "    slope   ca thal  \n",
       "285   3.0  3.0  6.0  \n",
       "193   2.0  3.0  3.0  \n",
       "266   2.0  0.0  NaN  \n",
       "279   2.0  0.0  3.0  \n",
       "120   1.0  2.0  7.0  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp  trestbps   chol  fbs restecg  thalach exang  oldpeak  \\\n",
       "143  64.0  1.0  3.0     125.0  309.0  0.0     0.0    131.0   1.0      1.8   \n",
       "92    NaN  1.0  3.0     130.0  231.0  0.0     0.0    146.0   0.0      1.8   \n",
       "21   58.0  0.0  1.0     150.0  283.0  1.0     2.0    162.0   0.0      1.0   \n",
       "185  63.0  0.0  2.0     140.0  195.0  0.0     0.0    179.0   0.0      0.0   \n",
       "257  76.0  0.0  3.0     140.0  197.0  0.0     1.0    116.0   0.0      1.1   \n",
       "\n",
       "    slope   ca thal  \n",
       "143   2.0  0.0  7.0  \n",
       "92    2.0  3.0  7.0  \n",
       "21    1.0  0.0  3.0  \n",
       "185   1.0  2.0  3.0  \n",
       "257   2.0  0.0  3.0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "X_val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the classification task easier, transform the target variable into a binary variable.\n",
    "# If the target variable is 0, it should remain 0. If the target variable is different from 0, it should be transformed into 1.\n",
    "y_train = pd.DataFrame((y_train != 0).astype(int), columns=[\"num\"])\n",
    "y_val = pd.DataFrame((y_val != 0).astype(int), columns=[\"num\"])\n",
    "y_test = pd.DataFrame((test[\"num\"] != 0).astype(int), columns=[\"num\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_set_from_franco = pd.read_csv('../../../testing_data/train.csv')\\nval_set_from_franco = pd.read_csv('../../../testing_data/val.csv')\\n\\nassert train.equals(train_set_from_franco), 'train set is not correct'\\nassert val.equals(val_set_from_franco), 'validation set is not correct'\\n\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "\n",
    "'''\n",
    "train_set_from_franco = pd.read_csv('../../../testing_data/train.csv')\n",
    "val_set_from_franco = pd.read_csv('../../../testing_data/val.csv')\n",
    "\n",
    "assert train.equals(train_set_from_franco), 'train set is not correct'\n",
    "assert val.equals(val_set_from_franco), 'validation set is not correct'\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: impute the missing values in the categorical columns. Use a KNNImputer from sklearn for the imputation process. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Create a subset of the train dataset with only the categorical columns. Call this subset train_cat.\n",
    "# 2. Create a subset of the val dataset with only the categorical columns. Call this subset val_cat.\n",
    "# 3. Create a subset of the test dataset with only the categorical columns. Call this subset test_cat\n",
    "# 4. Impute the three datasets using a KNN imputer with k=5 and weights set to distance\n",
    "# 5. Save the results in train_imputed_knn, val_imputed_knn, and test_imputed_knn.\n",
    "# 6. Make sure to add the column names to the resulting dataframes. DO NOT SKIP THIS STEP.\n",
    "# The new values might have new values that are not in the original dataset.\n",
    "# Approximate them to the nearest value in the original dataset, for each column.\n",
    "# To do so, you can store the original values of each column in a dictionary or a list.\n",
    "# if a new value is equidistant from two original values, choose the largest one.\n",
    "# (Example: if the original values are [1, 3] and the new value is 2, it will become 3)\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# Step 1-3\n",
    "train_cat = train[categorical_columns]\n",
    "val_cat = val[categorical_columns]\n",
    "test_cat = test[categorical_columns]\n",
    "                                  \n",
    "train_cat_encoded = train_cat.apply(lambda x: x.astype('category').cat.codes)\n",
    "val_cat_encoded = val_cat.apply(lambda x: x.astype('category').cat.codes)\n",
    "test_cat_encoded = test_cat.apply(lambda x: x.astype('category').cat.codes)\n",
    "\n",
    "#  Step 4\n",
    "knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "\n",
    "# Step 5 and 6\n",
    "train_imputed_knn = pd.DataFrame(knn_imputer.fit_transform(train_cat), columns=train_cat.columns)\n",
    "val_imputed_knn = pd.DataFrame(knn_imputer.transform(val_cat), columns=val_cat.columns)\n",
    "test_imputed_knn = pd.DataFrame(knn_imputer.transform(test_cat), columns=test_cat.columns)\n",
    "\n",
    "# Test the results\n",
    "# print(train_imputed_knn.head(3))\n",
    "# print(val_imputed_knn.head(3))\n",
    "# print(test_imputed_knn.head(3))\n",
    "\n",
    "# The new values might have new values that are not in the original dataset.\n",
    "# Approximate them to the nearest value in the original dataset, for each column.\n",
    "# To do so, you can store the original values of each column in a dictionary or a list.\n",
    "# if a new value is equidistant from two original values, choose the largest one.\n",
    "\n",
    "original_values = {col: sorted(train_cat_encoded[col].dropna().unique()) for col in train_cat_encoded.columns}\n",
    "\n",
    "\n",
    "def approximate_to_nearest(original_values, imputed_values):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_values : dict\n",
    "        A dictionary where the keys are column names, and the values are sorted lists\n",
    "        of unique numeric values present in the original dataset for each column. \n",
    "        These values represent the valid numeric codes for the categorical features.\n",
    "    \n",
    "    imputed_values : pd.DataFrame\n",
    "        A DataFrame containing the imputed values (from KNN imputation) for \n",
    "        categorical features. These values will be approximated to the nearest valid\n",
    "        value from the `original_values` dictionary.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with the imputed values replaced by the closest valid values \n",
    "        from the original dataset for each categorical feature.\n",
    "    \"\"\"\n",
    "    def find_nearest(original_list, value):\n",
    "        \n",
    "        original_array = np.array(original_list, dtype=float)  # Ensuring numeric dtype\n",
    "        abs_diff = np.abs(original_array - value)\n",
    "        \n",
    "        # Find the index of the closest value\n",
    "        idx_min = np.argmin(abs_diff)\n",
    "        \n",
    "        # If equidistant, choose the larger value\n",
    "        if (idx_min > 0) and (abs_diff[idx_min] == abs_diff[idx_min - 1]):\n",
    "            return original_array[idx_min]\n",
    "        \n",
    "        return original_array[idx_min]\n",
    "    \n",
    "    # Apply the approximation to each column\n",
    "    for col in imputed_values.columns:\n",
    "        imputed_values[col] = imputed_values[col].apply(lambda x: find_nearest(original_values[col], x))\n",
    "    \n",
    "    return imputed_values\n",
    "\n",
    "\n",
    "train_imputed_knn = approximate_to_nearest(original_values, train_imputed_knn.copy())\n",
    "val_imputed_knn = approximate_to_nearest(original_values, val_imputed_knn.copy())\n",
    "test_imputed_knn = approximate_to_nearest(original_values, test_imputed_knn.copy())\n",
    "\n",
    "# Check the results\n",
    "# print(train_imputed_knn.head())\n",
    "# print(val_imputed_knn.head())\n",
    "# print(test_imputed_knn.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_imp_knn_franco = pd.read_csv('../../../testing_data/train_imp_knn.csv')\\nval_imp_knn_franco = pd.read_csv('../../../testing_data/val_imp_knn.csv')\\ntest_imp_knn_franco = pd.read_csv('../../../testing_data/test_imp_knn.csv')\\n\\nassert train_imputed_knn.equals(train_imp_knn_franco), 'train imputed knn is not correct'\\nassert val_imputed_knn.equals(val_imp_knn_franco), 'val imputed knn is not correct'\\nassert test_imputed_knn.equals(test_imp_knn_franco), 'test imputed knn is not correct'\\n\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "'''\n",
    "train_imp_knn_franco = pd.read_csv('../../../testing_data/train_imp_knn.csv')\n",
    "val_imp_knn_franco = pd.read_csv('../../../testing_data/val_imp_knn.csv')\n",
    "test_imp_knn_franco = pd.read_csv('../../../testing_data/test_imp_knn.csv')\n",
    "\n",
    "assert train_imputed_knn.equals(train_imp_knn_franco), 'train imputed knn is not correct'\n",
    "assert val_imputed_knn.equals(val_imp_knn_franco), 'val imputed knn is not correct'\n",
    "assert test_imputed_knn.equals(test_imp_knn_franco), 'test imputed knn is not correct'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *4.* Imputing numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: impute the missing values in the numerical columns. Use a Lasso Regression from sklearn for the imputation process. `\n",
    "If more than one column contains missing values, proceed in increasing order: the lowest number of missing values first, then the second lowest, then the third ...\n",
    "\n",
    "Exclude the columns with missing values when fitting your regressor: only train on columns without missing values. After a column has been imputed, it can be used to fit the regressor in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of *_num:  212 91 117\n",
      "Amount of missing ages per dataset:  2 2 7\n",
      "Lengths of *_num_missign:  2 2 7\n",
      "Lengths of *_num_not_missign:  210 89 110\n",
      "Missing counts for train, val, test: \n",
      " trestbps    0\n",
      "chol        0\n",
      "thalach     0\n",
      "oldpeak     0\n",
      "age         2\n",
      "dtype: int64 trestbps    0\n",
      "chol        0\n",
      "thalach     0\n",
      "oldpeak     0\n",
      "age         2\n",
      "dtype: int64 trestbps    0\n",
      "chol        0\n",
      "thalach     0\n",
      "oldpeak     0\n",
      "age         7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Create a subset of the train dataset with only the numerical columns. Call this subset train_num.\n",
    "# 2. Create a subset of the val dataset with only the numerical columns. Call this subset val_num.\n",
    "# 3. Create a subset of the test dataset with only the numerical columns. Call this subset test_num.\n",
    "# 4a. Create a subset of train_num containing the rows with missing values. Call this subset train_num_missing.\n",
    "# 4b. Create a subset of train_num containing the rows without missing values. Call this subset train_num_not_missing.\n",
    "# 5a. Create a subset of val_num containing the rows with missing values. Call this subset val_num_missing.\n",
    "# 5b. Create a subset of val_num containing the rows without missing values. Call this subset val_num_not_missing.\n",
    "# 6a. Create a subset of test_num containing the rows with missing values. Call this subset test_num_missing.\n",
    "# 6b. Create a subset of test_num containing the rows without missing values. Call this subset test_num_not_missing.\n",
    "# 7. Using a Lasso regression, impute the missing values in train_num_missing, val_num_missing, and test_num_missing.\n",
    "# On what should the Lasso regression be trained?\n",
    "# 8. Repeat steps 1-7 until all the missing values are imputed.\n",
    "# 9. Save the results in train_num_imputed_lasso, val_num_imputed_lasso, and test_num_imputed_lasso.\n",
    "# 10. Concatenate the imputed subsets with the subsets that did not contain missing values.\n",
    "# 11. Save the resulting datasets in train_imputed_lasso, val_imputed_lasso, and test_imputed_lasso.\n",
    "# IMPORTANT: The order of the rows should be the same as in the original datasets.\n",
    "\n",
    "# Step 1-3\n",
    "train_num = train[numerical_columns_float]\n",
    "val_num = val[numerical_columns_float]\n",
    "test_num = test[numerical_columns_float]\n",
    "\n",
    "def split_missing_data(df):\n",
    "    missing_rows = df[df.isna().any(axis=1)]\n",
    "    not_missing_rows = df.dropna() \n",
    "    return missing_rows, not_missing_rows\n",
    "\n",
    "print(\"Lengths of *_num: \", len(train_num), len(val_num), len(test_num))\n",
    "\n",
    "# Step 4, 5, 6\n",
    "train_num_missing, train_num_not_missing = split_missing_data(train_num)\n",
    "val_num_missing, val_num_not_missing = split_missing_data(val_num)\n",
    "test_num_missing, test_num_not_missing = split_missing_data(test_num)\n",
    "\n",
    "# Get some information about the missing values \n",
    "print(\"Amount of missing ages per dataset: \", train_num['age'].isna().sum(), val_num['age'].isna().sum(), test_num['age'].isna().sum())\n",
    "print(\"Lengths of *_num_missign: \", len(train_num_missing), len(val_num_missing), len(test_num_missing))\n",
    "print(\"Lengths of *_num_not_missign: \", len(train_num_not_missing), len(val_num_not_missing), len(test_num_not_missing))\n",
    "\n",
    "# Check the amount of missing values overall\n",
    "missing_counts_train = train_num.isna().sum().sort_values()\n",
    "missing_counts_val = val_num.isna().sum().sort_values()\n",
    "missing_counts_test = test_num.isna().sum().sort_values()\n",
    "print(\"Missing counts for train, val, test: \\n\", missing_counts_train, missing_counts_val, missing_counts_test)\n",
    "\n",
    "# We can see that only ages are missing -> only this row needs imputation which is advantageous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "# 7. Using a Lasso regression, impute the missing values in train_num_missing, val_num_missing, and test_num_missing.\n",
    "# On what should the Lasso regression be trained?\n",
    "# 8. Repeat steps 1-7 until all the missing values are imputed.\n",
    "# 9. Save the results in train_num_imputed_lasso, val_num_imputed_lasso, and test_num_imputed_lasso.\n",
    "# 10. Concatenate the imputed subsets with the subsets that did not contain missing values.\n",
    "# 11. Save the resulting datasets in train_imputed_lasso, val_imputed_lasso, and test_imputed_lasso.\n",
    "# IMPORTANT: The order of the rows should be the same as in the original datasets.\n",
    "\n",
    "def lasso_impute(train_data, missing_data, update_train_data=True):\n",
    "    # Copy data to avoid modifying originals\n",
    "    train_data = train_data.copy()\n",
    "    missing_data = missing_data.copy()\n",
    "    \n",
    "    # While there are missing values\n",
    "    while missing_data.isnull().values.any():\n",
    "        # Get columns with missing values\n",
    "        cols_with_missing = missing_data.columns[missing_data.isnull().any()].tolist()\n",
    "        #print(\"cols with missing values: \", cols_with_missing)\n",
    "        # Sort columns by the number of missing values\n",
    "        missing_counts = missing_data[cols_with_missing].isnull().sum().sort_values()\n",
    "        cols_sorted = missing_counts.index.tolist()\n",
    "        \n",
    "        for col in cols_sorted:\n",
    "            # Prepare training data (use non-missing rows)\n",
    "            X_train = train_data.drop(columns=[col]).dropna()\n",
    "            y_train = train_data.loc[X_train.index, col]\n",
    "            y_train = y_train.dropna()\n",
    "            X_train = X_train.loc[y_train.index]\n",
    "            # index = missing_data[missing_data['age'].isna()]\n",
    "            # print(index)\n",
    "            # Prepare prediction data\n",
    "            X_pred = missing_data.loc[missing_data[col].isnull(), X_train.columns]\n",
    "            # Drop rows with missing values in X_pred\n",
    "            X_pred = X_pred.dropna()\n",
    "            \n",
    "            # If no data to train or predict, continue to next column\n",
    "            if X_train.empty or X_pred.empty:\n",
    "                continue\n",
    "            \n",
    "            # Fit Lasso regression\n",
    "            lasso = Lasso()\n",
    "            lasso.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict missing values\n",
    "            y_pred = lasso.predict(X_pred)\n",
    "            # Impute the predicted values\n",
    "            missing_data.loc[X_pred.index, col] = y_pred\n",
    "            \n",
    "            if update_train_data:\n",
    "                # Assign imputed values back to train_data\n",
    "                train_data.loc[X_pred.index, col] = y_pred\n",
    "                \n",
    "                # Remove imputed rows from missing_data\n",
    "                missing_data = missing_data.drop(index=X_pred.index)\n",
    "    \n",
    "    if update_train_data:\n",
    "        return train_data\n",
    "    else:\n",
    "        return missing_data\n",
    "\n",
    "# Step 9: \n",
    "\n",
    "# Use the entire train_num as train_data\n",
    "train_num_imputed_lasso = lasso_impute(train_num, train_num_missing, update_train_data=True)\n",
    "\n",
    "# For val and test, use the imputed train data to train the lasso function on \n",
    "val_num_imputed_lasso = lasso_impute(train_num_imputed_lasso, val_num_missing, update_train_data=False)\n",
    "test_num_imputed_lasso = lasso_impute(train_num_imputed_lasso, test_num_missing, update_train_data=False)\n",
    "#print(\"Validation:\", val_num_imputed_lasso['age'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added this cell to split the computation of the Lasso and the order of the cells for easier reading \n",
    "# IMPORTANT: The order of the rows should be the same as in the original datasets.\n",
    "\n",
    "# For train set\n",
    "train_imputed_lasso = train.copy()\n",
    "train_imputed_lasso[numerical_columns_float] = train_num_imputed_lasso[numerical_columns_float]\n",
    "\n",
    "# For validation set\n",
    "val_imputed_lasso = val.copy()\n",
    "nan_rows = val_imputed_lasso[val_imputed_lasso['age'].isna()]\n",
    "val_imputed_lasso.loc[val_num_imputed_lasso.index, numerical_columns_float] = val_num_imputed_lasso[numerical_columns_float]\n",
    "\n",
    "# For test set\n",
    "test_imputed_lasso = test.copy()\n",
    "test_imputed_lasso.loc[test_num_imputed_lasso.index, numerical_columns_float] = test_num_imputed_lasso[numerical_columns_float]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'age' after imputation:\n",
      "Train: 0\n",
      "Validation: 0\n",
      "Test: 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing counts for train, val, test: \n",
      " age         0\n",
      "trestbps    0\n",
      "chol        0\n",
      "thalach     0\n",
      "oldpeak     0\n",
      "dtype: int64 age         0\n",
      "trestbps    0\n",
      "chol        0\n",
      "thalach     0\n",
      "oldpeak     0\n",
      "dtype: int64 age         0\n",
      "trestbps    0\n",
      "chol        0\n",
      "thalach     0\n",
      "oldpeak     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_counts_train = train_num_imputed_lasso[numerical_columns_float].isna().sum().sort_values()\n",
    "missing_counts_val = val_num_imputed_lasso[numerical_columns_float].isna().sum().sort_values()\n",
    "missing_counts_test = test_num_imputed_lasso[numerical_columns_float].isna().sum().sort_values()\n",
    "print(\"Missing counts for train, val, test: \\n\", missing_counts_train, missing_counts_val, missing_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_imp_lasso_franco = pd.read_csv('../../../testing_data/train_imp_lasso.csv')\\nval_imp_lasso_franco = pd.read_csv('../../../testing_data/val_imp_lasso.csv')\\ntest_imp_lasso_franco = pd.read_csv('../../../testing_data/test_imp_lasso.csv')\\n\\nassert train_imputed_lasso.equals(train_imp_lasso_franco), 'train imputed lasso is not correct'\\nassert val_imputed_lasso.equals(val_imp_lasso_franco), 'val imputed lasso is not correct'\\nassert test_imputed_lasso.equals(test_imp_lasso_franco), 'test imputed lasso is not correct'\\n\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "'''\n",
    "train_imp_lasso_franco = pd.read_csv('../../../testing_data/train_imp_lasso.csv')\n",
    "val_imp_lasso_franco = pd.read_csv('../../../testing_data/val_imp_lasso.csv')\n",
    "test_imp_lasso_franco = pd.read_csv('../../../testing_data/test_imp_lasso.csv')\n",
    "\n",
    "assert train_imputed_lasso.equals(train_imp_lasso_franco), 'train imputed lasso is not correct'\n",
    "assert val_imputed_lasso.equals(val_imp_lasso_franco), 'val imputed lasso is not correct'\n",
    "assert test_imputed_lasso.equals(test_imp_lasso_franco), 'test imputed lasso is not correct'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *5.* Classification with Decision Tree, using a single split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the train_imputed_knn and train_imputed_lasso datasets. Call the resulting dataset X_train_imputed.\n",
    "\n",
    "X_train_imputed = pd.concat([train_imputed_knn, train_imputed_lasso], axis=1)\n",
    "# Merge the val_imputed_knn and val_imputed_lasso datasets. Call the resulting dataset X_val_imputed.\n",
    "X_val_imputed = pd.concat([val_imputed_knn, val_imputed_lasso], axis=1)\n",
    "# Merge the test_imputed_knn and test_imputed_lasso datasets. Call the resulting dataset X_test_imputed.\n",
    "X_test_imputed = pd.concat([test_imputed_knn, test_imputed_lasso], axis=1)\n",
    "\n",
    "# drop the num column such that the datasets can be used for training \n",
    "X_train_imputed = X_train_imputed.drop(columns=['num'])\n",
    "X_val_imputed = X_val_imputed.drop(columns=['num'])\n",
    "X_test_imputed = X_test_imputed.drop(columns=['num'])\n",
    "\n",
    "# Sanity checks: \n",
    "\n",
    "assert len(X_train_imputed) == (len(train))\n",
    "assert len(X_val_imputed) == (len(val))\n",
    "assert len(X_test_imputed) == (len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Train a set of Decision Trees, using different hyperparameters. Use the best performing Decision Tree to predict the class for the test set. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to run the hyperparameter tuning with a single split:  0.06059384346008301\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a dictionary to contain the hyperparameters. The dictionary should contain the following:\n",
    "# - criterion: 'gini' and 'entropy'\n",
    "# - max_depth: 3, 5, and 7\n",
    "# - min_samples_split: 2, 5, and 10\n",
    "# 2. Create a dictionary called performance to store the hyperparameter combinations and the corresponding performance of the model.\n",
    "# 3. Create a ParameterGrid object with the hyperparameters from the dictionary.\n",
    "# 4. Create a for loop to iterate over the combinations of hyperparameters.\n",
    "# 5. In each iteration\n",
    "# - Create a DecisionTreeClassifier with the hyperparameters for that iteration.\n",
    "# - Fit the model.\n",
    "# - Predict the target variable for the validation set.\n",
    "# - Calculate the F1 score of the model.\n",
    "# - Add the hyperparameters and the F1 score to the performance dictionary.\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "# Step 1\n",
    "hyperparameters = {'max_depth': [3, 5, 7], 'min_samples_split': [2, 5, 10], 'criterion': ['gini', 'entropy']} \n",
    "# Step 2\n",
    "performance = {}\n",
    "# Step 3\n",
    "param_grid = ParameterGrid(hyperparameters)\n",
    "\n",
    "start = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "# Step 4\n",
    "for i, parameter_tree_combination in enumerate(param_grid):\n",
    "    # Step 5.1\n",
    "    tree = DT(**parameter_tree_combination)\n",
    "    # Step 5.2\n",
    "    tree.fit(X_train_imputed, y_train)\n",
    "    # Step 5.3 predict the target variable for the validation set \n",
    "    y_tree = tree.predict(X_val_imputed)\n",
    "    # Step 5.4 Calculate the F1 score of the model\n",
    "    f1 = f1_score(y_val, y_tree, average='macro')\n",
    "    # Step 5.5 Use index 'i' as the key to add the F1 score to the perfomance dictionary \n",
    "    performance[i] = {\n",
    "        'parameters': parameter_tree_combination,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "end = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "print('Time elapsed to run the hyperparameter tuning with a single split: ', end - start) # DO NOT CHANGE/DELETE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 2}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the best performing hyperparameters\n",
    "best_index = max(performance, key=lambda i: performance[i]['f1_score'])  \n",
    "\n",
    "best_hyperparameters = performance[best_index]['parameters']  \n",
    "\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4126605972943078\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the train and validation datasets. Call the resulting datasets X and y.\n",
    "X = pd.concat([X_train_imputed, X_val_imputed])\n",
    "y = pd.concat([y_train, y_val])\n",
    "\n",
    "# Create a DecisionTreeClassifier with the best hyperparameters.\n",
    "best_params = {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 2}\n",
    "tree = DT(**best_params)\n",
    "\n",
    "# Fit the model on the X and y datasets.\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Predict the target variable for the test dataset.\n",
    "y_test_pred = tree.predict(X_test_imputed)\n",
    "\n",
    "# Calculate the F1 score of the model on the test dataset. Call the variable f1_test_single_split.\n",
    "f1_test_single_split = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f1_test_single_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4126605972943078)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_test_single_split # DO NOT DELETE/CHANGE THIS LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *6.* Classification with Decision Tree using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Train a cross-validation object, then train a decision tree using cross-validation and different hyperparameters. Use the best performing Decision Tree to predict the class for the test set. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to run the hyperparameter tuning with Cross Validation:  0.34484076499938965\n"
     ]
    }
   ],
   "source": [
    "# 1. Use the same hyperparameters from the previous task which is: \n",
    "# {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 2}\n",
    "# 2. Create a StratifiedKFold object with 5 splits, use shuffle=True.\n",
    "# 3. Create a dictionary called performance_CV to store the hyperparameter combinations and the corresponding performance of the model.\n",
    "# 3. Create a ParameterGrid object with the usual hyperparameters.\n",
    "# 4. Create a for loop to iterate over the folds of the StratifiedKFold.\n",
    "# 5. For each fold, create a for loop to iterate over the combinations of hyperparameters.\n",
    "# 6. In each iteration\n",
    "# - Create a DecisionTreeClassifier with the hyperparameters for that iteration.\n",
    "# - Fit the model.\n",
    "# - Predict the target variable for the validation fold.\n",
    "# - Calculate the F1 score of the model.\n",
    "# - Add the hyperparameters and the F1 score to the performance dictionary. Each hyperparameter combination may have multiple F1 scores.\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = pd.concat([X_train_imputed, X_val_imputed])\n",
    "y = pd.concat([y_train, y_val])\n",
    "\n",
    "# DO NOT FORGET TO DELETE THE PREVIOUS LINES. They are only to make the empty assignment run without errors,\n",
    "# but they will destroy the data you need.\n",
    "\n",
    "# Step 1\n",
    "hyperparameters = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "# Step 2\n",
    "CV = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# Step 3\n",
    "performance_CV = defaultdict(list)\n",
    "param_grid = ParameterGrid(hyperparameters)\n",
    "\n",
    "start_CV = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "# Step 4\n",
    "for i, (train_index, val_index) in enumerate(CV.split(X, y)):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # 5. For each fold, create a for loop to iterate over the combinations of hyperparameters.\n",
    "    for params in param_grid:\n",
    "        # Step 6\n",
    "        # Create a DecisionTreeClassifier with the hyperparameters for that iteration.\n",
    "        tree = DecisionTreeClassifier(**params)\n",
    "        # Fit the model.\n",
    "        tree.fit(X_train_fold, y_train_fold)\n",
    "        # Predict the target variable for the validation fold.\n",
    "        y_pred = tree.predict(X_val_fold)\n",
    "        # Calculate the F1 score.\n",
    "        f1 = f1_score(y_val_fold, y_pred, average='macro')\n",
    "        # print(f\"F1 = {f1} for params: {params}\")\n",
    "        # Add the hyperparameters and the F1 score to the performance dictionary.\n",
    "        params_key = tuple(sorted(params.items()))\n",
    "        # print(\"params key: \", params_key)\n",
    "        performance_CV[params_key].append(f1)\n",
    "\n",
    "end_CV = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "print('Time elapsed to run the hyperparameter tuning with Cross Validation: ', end_CV - start_CV) # DO NOT CHANGE/DELETE THIS LINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('criterion', 'entropy'), ('max_depth', 3), ('min_samples_split', 2))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the best performing hyperparameters, which are the ones with the highest average F1 score\n",
    "best_hyperparameters_CV = max(performance_CV, key=lambda params: np.mean(performance_CV[params]))\n",
    "\n",
    "best_hyperparameters_CV # DO NOT DELETE/CHANGE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DecisionTreeClassifier with the best hyperparameters.\n",
    "# Fit the model on the X and y datasets.\n",
    "best_params_dict = dict(best_hyperparameters_CV)\n",
    "# Call the fitted model final_tree.\n",
    "final_tree = DecisionTreeClassifier(**best_params_dict)\n",
    "\n",
    "final_tree.fit(X, y)\n",
    "# Predict the target variable for the test dataset.\n",
    "y_test_pred = final_tree.predict(X_test_imputed)\n",
    "\n",
    "# Calculate the F1 score of the model on the test dataset. Call the variable f1_test_CV.\n",
    "f1_test_CV = f1_score(y_test, y_test_pred, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4900726392251816)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_test_CV # DO NOT DELETE/CHANGE THIS LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *7.* Interpretation of the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Look at the times elapsed to train the Decision Tree using the single split and the CV strategies. Is there a difference? Explain the difference or the lack of difference in 50 words or less. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time elapsed for single split: 0.06707382202148438, time elapsed for CV: 0.29927706718444824. This is nearly a 3 time increase in training time. Which is also resonable, as the CV strategy includes training 5 times as many decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Plot final_tree, and explain which feature or combination of features is the most relevant for that model, in 50 words or less. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find instructions on how to plot a decision tree at [this link](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your tree here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your explanation here. Delete this text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
